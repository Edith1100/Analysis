{"cells":[{"cell_type":"markdown","source":["# Eksploracyjna analiza danych (EDA) z wykorzystaniem PySparka"],"metadata":{}},{"cell_type":"markdown","source":["Eksploracyjna analiza danych jest jedną z najważniejszych części każdego procedu analizy. Bez prawidłowo opisanych danych, a  następnie oczyszczonych - proces analizy nie może zakończyć się sukcesem.\n\nMożna ją podzielić na kilka stadiów:\n\n1. Eksploracja dancyh:\n\n    1. Przegląd typów danych\n    2. Przegląd rozkładu wartości, w tym wartości odstających\n    3. Przegląd wartości brakujących lub błędów systematyczncyh\n\n2. Oczyszczanie danych:\n    1. Uzupełnianie wartości brakujących\n    2. Obsługa wartości odstających w zbirze."],"metadata":{}},{"cell_type":"markdown","source":["# Analiza wartości brakujących"],"metadata":{}},{"cell_type":"code","source":["df_miss = spark.createDataFrame([ (1, 143.5, 5.6, 28,\n'M', 100000),\n(2, 167.2, 5.4, 45, 'M', None),\n(3, None , 5.2, None, None, None),\n(4, 144.5, 5.9, 33, 'M', None),\n(5, 133.2, 5.7, 54, 'F', None),\n(6, 124.1, 5.2, None, 'F', None),\n(7, 129.2, 5.3, 42, 'M', 76000),\n], ['id', 'weight', 'height', 'age', 'gender', 'income'])"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df_miss.show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Wyszukiwanie obserwacji z brakującymi wartościami - przejście na **RDD** i analiza, których elementów brakuje.\n\nPodstawową operacją będzie zliczenie brakujących wartości w wierszach.\n\nStruktura kodu:\n\n\n```{python}\n\ndata_frame.rdd.map(lambda row: ANALIZA)\n\n```"],"metadata":{}},{"cell_type":"code","source":["# Analizujemy ilośc brakujących wartości w każdym wieszu - suma brakujących pól\n\ndf_miss.rdd.map(\n\tlambda row: (row['id'], sum([c == None for c in row]))\n).collect()\n\n# Oczekiwany wynik: pary uporządkowane: (id_wiersza, SUMA wartości brakujących), np.: (1, 10), (2, 0)\n"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Analiza procentowa udziału wartości brakujących w całym zbiorze - analiza wg. każdej kolumny po kolei.\n\nSkładnia polecenia:\n\n```{python}\n\nimport pyspark.sql.functions as fn\n\ndata_frame.agg(\n  (1- (fn.count( KOLUMNA )/fn.count('*')))\n).show()\n\n```"],"metadata":{}},{"cell_type":"code","source":["import pyspark.sql.functions as fn\n\ndf_miss.where('id == 3').show()\n\n\n# Oczekiwany wynik - procentowy udział wartości brakujących w całym zbiorze"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# To samo dla wielu kolumn - iteracja po wszystkich wartościach\n\ndf_miss.agg(*[\n (1 - (fn.count(c)/fn.count('*'))).alias(c + '_missing')\n  for c in df_miss.columns\n]).show()\n\n# Oczekiwany wynik - procentowy udział wartości brakujących w każdej kolumnie\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# Wiersze, w których brakuje więcej niż X wartości można po prostu opuścić - usunąć ze zbioru danych. Poleceniem dropna(threshold=X)\n\ndf_miss_with_income = df_miss.dropna(thresh=3)\ndf_miss_with_income.show()\n\n# Oczekiwany wynik - zbiór danych bez wierszy, w których brakuje co najmniej 3 wartości"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Usunięcie kolumny income, ze względu na zbyt duży procent obserwacji brakujących\n\ndf_miss_no_income = df_miss_with_income.select([ c for c in df_miss.columns if c != 'income'])\ndf_miss_no_income.show()\n"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Usuwanie wartości brakujących nie jest najlepszym sposobem. Zazwyczaj można jakoś je uzupełnić. Są na to co najmniej dwa sposoby:\n\n1. Można użyć stałej wartości (tzw. sposób wartości \"magicznej\") - np. -1 albo 0.\n2. Można użyć jakiejś wartości charakterystycznej dla danej kolumny -np. średniej/mediany itp."],"metadata":{}},{"cell_type":"code","source":["# Sposób 1 - wstawianie stałej wartości\n\nmeans = df_miss_no_income.agg(\n     *[fn.mean(c).alias(c)\n          for c in df_miss_no_income.columns if c != 'gender']\n).toPandas().to_dict('records')[0]\n\nmeans['age'] = '0'\ndf_miss_no_income.fillna(means).show()\n\n\n# Oczekiwany wynik - zbiór danych, gdzie brakujące wartości uzupełniono zerami"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Sposób 2 - użycie średniej\n\n# Krok 1 - znajdujemy średnią wartości dla każdej kolumny po kolei\n\nmeans2 = df_miss_no_income.agg(\n     *[fn.mean(c).alias(c)\n          for c in df_miss_no_income.columns if c != 'gender'])\n\nmeans2.show()\n\n\n# Oczekiwany wynik - zbiór danych, gdzie wyliczono średnią wartośc dla każdej kolumny (z wyjątkiem 'gender')"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Krok 2 -zamiana zbioru danych średnich na słownik \n\nmeans_dict = means2.toPandas().to_dict('records')[0]\nmeans_dict\n\n# Oczekiwany wynik - słownik, gdzie dla każdej kolumny umieszczono średnią wartośc. Dla kolumny gender -wstawic '??'"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Krok 3 - wykorzystanie słownika w funkcji fillna\n\ndf_miss_no_income.fillna(means_dict).show()\n\n# Oczekiwany wynik - zbiór danych uzupełniony średnimi z poprzedniego punktu\n"],"metadata":{},"outputs":[],"execution_count":17}],"metadata":{"name":"Analiza z użyciem PySpark","notebookId":2049030226271534},"nbformat":4,"nbformat_minor":0}
